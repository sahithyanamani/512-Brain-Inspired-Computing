{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Goals\n",
    "In Assignment 2, we learnt how to construct networks of spiking neurons and propagate information through a network of fixed weights. In this assignment, you will learn how to train network weights for a given task using brain-inspired learning rules.\n",
    "\n",
    "Let's import all the libraries required for this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Training a Network\n",
    "\n",
    "## 1a. \n",
    "What is the purpose of a learning algorithm? In other words, what does a learning algorithm dictate, and what is the objective of it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1a. \n",
    "The learning algorithms helps us train a machine learning model and make predictions based on that learning. It also dictates how a model updates its parameters or weights based on the input data to reduce the difference between the models outputs and targeted outputs or actual outputs. The target of a learning algorithm is to reduce the prediction error on new or unseen data. In other words, the purpose of the learning algorithm is to make the model learn on the input data and improve its performance without being explicitly programmed to do so. A learning algorithm specifies how and under what conditions a learning rule or a combination of learning rules should be applied to adjust the network parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. \n",
    "Categorize and explain the various learning algorithms w.r.t. biological plausibility. Can you explain the tradeoffs involved with the different learning rules? *Hint: Think computational advantages and disadvantages of biological plausibility.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1b. \n",
    "The learning algorithms can be divided into two categories one with rules that are biologically plausible and others that are not-biologically plausible:\n",
    "\n",
    "BIOLOGICAL PLAUSIBILITY: In Brain Inspired Computing biological plausibility means that the computational model or algorithm must be based on principles that are consistent with what we know as the functioning of biological neurons andetworks in the brain.\n",
    "\n",
    "a) Biological Plausible: \n",
    "- The weight change between a presynaptic and a postsynaptic neuron should only depend upon pre and post synaptic neuron joint activity, and also satisfy the constraint of locality.\n",
    "- Hebbian Learning algorithm and Spike Time Dependent Plasticity (STDP) are the algorithms which are biologically plausible.\n",
    "- Hebb's rule, Oja's rule, BCM rule and Covariance rules are biologically plausible rules. \n",
    "- Hebb's rule considers correlation between the pre and post synaptic firing rates. And with each epoch or iteration the weights keep growing. In a system where synapses can only be strengthened all efficacies will eventually saturate at their upper maximum value. And hebbain cannot learn on negative weights.\n",
    "- Oja‚Äôs rule converges asymptotically to synaptic weights that are normalized to ‚àë<sub>ùëó</sub>W<sup>2</sup><sub>ùëñùëó</sub> = 1 while keeping the essential Hebbian properties of the standard rule. We note that normalization of ‚àë<sub>ùëó</sub>W<sup>2</sup><sub>ùëñùëó</sub> implies competition between the synapses that make connections to the same postsynaptic neuron, i.e., if some weights grow, others must decrease.\n",
    "- In Bienenstock-Cooper-Munro (BCM) rule the synaptic plasticity is characterized by two threshold for the postsynaptic activity, v<sub>0</sub> and v<sub>$\\theta$</sub>. If presynaptic activity is combined with moderate levels of postsynaptic excitation, the efficacy of synapses activated by presynaptic input is decreased . Weights are increased only if the level of postsynaptic activity exceeds a threshold, v<sub>$\\theta$</sub>. The change of weights is restricted to those synapses which are activated by presynaptic inputBelow v<sub>0</sub> no synaptic modification occurs, between v<sub>0</sub> and v<sub>$\\theta$</sub> synapses are depressed, and for postsynaptic firing rates beyond v<sub>$\\theta$</sub> synaptic potentiation can be observed. The BCM rule leads to input selectivity.\n",
    "- The covariance rule calculates the covariance between the presynaptic and postsynaptic activities and uses this value to update the weight of the synapse. Specifically, the change in the weight of the synapse is proportional to the covariance between the presynaptic and postsynaptic activities. It is based on the idea that rates v<sub>i</sub> and v<sub>j</sub> fluctuate around mean values <v<sub>i</sub>>,<v<sub>j</sub>> that are taken as running averages over the recent firing history.\n",
    "- STDP's main idea is that it is the timing of spikes rather than the specific rate of spikes that carries neural information. This observation is indeed in agreement with Hebb‚Äôs postulate because presynaptic neurons that are active slightly before the postsynaptic neuron are those which take part in firing it whereas those that fire later did not contribute to the postsynaptic action potential.\n",
    "\n",
    "b) Non-Biologically palusible:\n",
    "- Non-Local connections can influence the wight change.\n",
    "- Backpropagation using Gardient Descent is Non-biologically plausible algorithm.\n",
    "- The backpropagation algorithm works by iteratively adjusting the weights of the connections between neurons in the network based on the error between the predicted output and the actual output. This process involves two main steps: forward propagation and backpropagation.\n",
    "\n",
    "In the forward propagation step, the input data is passed through the network and the output is calculated using the current weights of the connections between the neurons. This output is compared to the actual output, and the error between the two is calculated.\n",
    "\n",
    "In the backpropagation step, the error is propagated backwards through the network from the output layer to the input layer. The weights of the connections between the neurons are then adjusted based on the calculated error using gradient descent.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Hebbian Learning\n",
    "\n",
    "## 2a.\n",
    "\n",
    "In this exercise, you will implement the hebbian learning rule to solve AND Gate. First, we need to create a helper function to generate the training data. The function should return lists of tuples where each tuple comprises of numpy arrays of rate-coded inputs and the corresponding rate-coded output. \n",
    "\n",
    "Below is the function to generate the training data. Fill the components to return the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genANDTrainData(snn_timestep):\n",
    "    \"\"\" \n",
    "    Function to generate the training data for AND \n",
    "        Args:\n",
    "            snn_timestep (int): timesteps for SNN simulation\n",
    "        Return:\n",
    "            train_data (list): list of tuples where each tuple comprises of numpy arrays of rate-coded inputs and output\n",
    "        \n",
    "        Write the expressions for encoding 0 and 1. Then append all 4 cases of AND gate to the list train_data\n",
    "    \"\"\"\n",
    "    \n",
    "    #Initialize an empty list for train data\n",
    "    train_data = []\n",
    "    \n",
    "    #encode 0. Numpy random choice function might be useful here.\n",
    "    encode_0 = np.random.choice([0,1],snn_timestep, p=[0.7,0.3]) \n",
    "\n",
    "    \n",
    "    #encode 1. Numpy random choice function might be useful here.\n",
    "    encode_1 = np.random.choice([0,1],snn_timestep, p=[0.3,0.7])  \n",
    "\n",
    "    y = np.logical_and(encode_0, encode_1)\n",
    "    #Append all 4 cases of AND gate to train_data. Numpy stack operation might be useful here. \n",
    "    # e.g., [0,0], [1,0], [0,1], [1,1]\n",
    "    and_00 = [np.array([encode_0, encode_0]), encode_0]\n",
    "    and_01 = [np.array([encode_0, encode_1]), encode_0]\n",
    "    and_10 = [np.array([encode_1, encode_0]), encode_0]\n",
    "    and_11 = [np.array([encode_1, encode_1]), encode_1]\n",
    "    train_data = np.stack((and_00, and_01, and_10, and_11))\n",
    "\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. \n",
    "We will use the implementation of the network from assignment 2 to create an SNN comprising of one input layer and one output layer. Can you explain algorithmically, how you can use this simple architecture to learn AND gate. Your algorithm should comprise of encoding, forward propagation, network training, and decoding steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 2b. \n",
    "The algorithm consists of multiple steps like encoding, training, forward propagation and decoding:<br>\n",
    "\n",
    "1. Encoding:\n",
    "We encode 0 and 1 using rate-based encoding<br>Generate train dataset using the encoded values of 0 and 1 for all cases of AND gate:\n",
    "    - Case 1: 0 & 0 = 0\n",
    "    - Case 2: 0 & 1 = 0\n",
    "    - Case 3: 1 & 0 = 0\n",
    "    - Case 4: 1 & 1 = 1\n",
    "2. Training:\n",
    "Initialize the network with random weights\n",
    "    - Iterate over all examples in the training dataset: [[X<sub>1</sub>,X<sub>2</sub>],Y]\n",
    "        - Calculate Input Firing Rate for X<sub>1</sub>: r<sub>1</sub>\n",
    "        - Calculate Input Firing Rate for X<sub>2</sub>: r<sub>2</sub>\n",
    "        - Calculate Output Firing Rate for Y: r<sub>o</sub>\n",
    "        - Calculate correlation: corr<sub>X<sub>1</sub>,Y</sub> = r<sub>1</sub>*r<sub>o</sub>; &nbsp; &nbsp; corr<sub>X<sub>2</sub>,Y</sub> = r<sub>2</sub>*r<sub>o</sub>\n",
    "        - Update weight: w<sub>11</sub> = w<sub>11</sub> + $\\eta $ * corr<sub>X<sub>1</sub>,Y</sub>; &nbsp; &nbsp; w<sub>22</sub> = w<sub>22</sub> + $\\eta $ * corr<sub>X<sub>2</sub>,Y</sub>\n",
    "    - Repeat for specified number of epochs\n",
    "3. Forward Propagation:\n",
    "\n",
    "- Iterate over train dataset to propagate spikes through network using trained weights for the network<br>\n",
    "- For each training instance repeat for specified number of timesteps and accumulate output spikes, calculate output firing rate at the end of timestep\n",
    "4. Decoding:Decode the rate into classes [0, 1] using thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SNN has already been implemented for you. You do not need to do anything here. Just understand the implementation so that you can use it in the later parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIFNeurons:\n",
    "    \"\"\" \n",
    "        Define Leaky Integrate-and-Fire Neuron Layer \n",
    "        This class is complete. You do not need to do anything here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dimension, vdecay, vth):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dimension (int): Number of LIF neurons in the layer\n",
    "            vdecay (float): voltage decay of LIF neurons\n",
    "            vth (float): voltage threshold of LIF neurons\n",
    "        \n",
    "        \"\"\"\n",
    "        self.dimension = dimension\n",
    "        self.vdecay = vdecay\n",
    "        self.vth = vth\n",
    "\n",
    "        # Initialize LIF neuron states\n",
    "        self.volt = np.zeros(self.dimension)\n",
    "        self.spike = np.zeros(self.dimension)\n",
    "    \n",
    "    def __call__(self, psp_input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            psp_input (ndarray): synaptic inputs \n",
    "        Return:\n",
    "            self.spike: output spikes from the layer\n",
    "                \"\"\"\n",
    "        self.volt = self.vdecay * self.volt * (1. - self.spike) + psp_input\n",
    "        self.spike = (self.volt > self.vth).astype(float)\n",
    "        return self.spike\n",
    "\n",
    "class Connections:\n",
    "    \"\"\" Define connections between spiking neuron layers \"\"\"\n",
    "\n",
    "    def __init__(self, weights, pre_dimension, post_dimension):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            weights (ndarray): connection weights\n",
    "            pre_dimension (int): dimension for pre-synaptic neurons\n",
    "            post_dimension (int): dimension for post-synaptic neurons\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.pre_dimension = pre_dimension\n",
    "        self.post_dimension = post_dimension\n",
    "    \n",
    "    def __call__(self, spike_input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            spike_input (ndarray): spikes generated by the pre-synaptic neurons\n",
    "        Return:\n",
    "            psp: postsynaptic layer activations\n",
    "        \"\"\"\n",
    "        psp = np.matmul(self.weights, spike_input)\n",
    "        return psp\n",
    "    \n",
    "    \n",
    "class SNN:\n",
    "    \"\"\" Define a Spiking Neural Network with No Hidden Layer \"\"\"\n",
    "\n",
    "    def __init__(self, input_2_output_weight, \n",
    "                 input_dimension=2, output_dimension=2,\n",
    "                 vdecay=0.5, vth=0.5, snn_timestep=20):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_2_hidden_weight (ndarray): weights for connection between input and hidden layer\n",
    "            hidden_2_output_weight (ndarray): weights for connection between hidden and output layer\n",
    "            input_dimension (int): input dimension\n",
    "            hidden_dimension (int): hidden_dimension\n",
    "            output_dimension (int): output_dimension\n",
    "            vdecay (float): voltage decay of LIF neuron\n",
    "            vth (float): voltage threshold of LIF neuron\n",
    "            snn_timestep (int): number of timesteps for inference\n",
    "        \"\"\"\n",
    "        self.snn_timestep = snn_timestep\n",
    "        self.output_layer = LIFNeurons(output_dimension, vdecay, vth)\n",
    "        self.input_2_output_connection = Connections(input_2_output_weight, input_dimension, output_dimension)\n",
    "    \n",
    "    def __call__(self, spike_encoding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            spike_encoding (ndarray): spike encoding of input\n",
    "        Return:\n",
    "            spike outputs of the network\n",
    "        \"\"\"\n",
    "        spike_output = np.zeros(self.output_layer.dimension)\n",
    "        for tt in range(self.snn_timestep):\n",
    "            input_2_output_psp = self.input_2_output_connection(spike_encoding[:, tt])\n",
    "            output_spikes = self.output_layer(input_2_output_psp)\n",
    "            spike_output += output_spikes\n",
    "        return spike_output/self.snn_timestep      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c. \n",
    "Next, you need to write a function for network training using hebbian learning rule. The function is defined below. You need to fill in the components so that the network weights are updated in the right manner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hebbian(network, train_data, lr=1e-3, epochs=10):\n",
    "    \"\"\" \n",
    "    Function to train a network using Hebbian learning rule\n",
    "        Args:\n",
    "            network (SNN): SNN network object\n",
    "            train_data (list): training data \n",
    "            lr (float): learning rate\n",
    "            epochs (int): number of epochs to train with. Each epoch is defined as one pass over all training samples. \n",
    "        \n",
    "        Write the operations required to compute the weight increment according to the hebbian learning rule. Then increment the network weights. \n",
    "    \"\"\"\n",
    "    \n",
    "    #iterate over the epochs\n",
    "    for ee in range(epochs):\n",
    "        #iterate over all samples in train_data\n",
    "        for data in train_data:\n",
    "            #compute the firing rate for the input\n",
    "            v_i = np.sum(data[0])/len(data[1])\n",
    "            #compute the firing rate for the output\n",
    "            v_j = np.sum(data[1])/len(data[1])\n",
    "            #compute the correlation using the firing rates calculated above\n",
    "            correlation = v_i * v_j\n",
    "            #compute the weight increment\n",
    "            delta_w = lr * correlation\n",
    "            #increment the weight\n",
    "            network.input_2_output_connection.weights += delta_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d. \n",
    "In this exercise, you will use your implementations above to train an SNN to learn AND gate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights: [[0.06108384 0.10768687]]\n",
      "Interation 0:\n",
      "[[1 0 0 0 0 0 1 0 0 0]\n",
      " [1 0 0 0 0 0 1 0 0 0]]\n",
      "Output Spiking Rate: [0.2]\n",
      "Interation 1:\n",
      "[[1 0 0 0 0 0 1 0 0 0]\n",
      " [1 1 1 1 1 1 0 1 0 1]]\n",
      "Output Spiking Rate: [0.1]\n",
      "Interation 2:\n",
      "[[1 1 1 1 1 1 0 1 0 1]\n",
      " [1 0 0 0 0 0 1 0 0 0]]\n",
      "Output Spiking Rate: [0.1]\n",
      "Interation 3:\n",
      "[[1 1 1 1 1 1 0 1 0 1]\n",
      " [1 1 1 1 1 1 0 1 0 1]]\n",
      "Output Spiking Rate: [0.8]\n",
      "\n",
      "Prediction:  [0, 0, 0, 1]\n",
      "Final weights =  [[0.07868384 0.12528687]]\n"
     ]
    }
   ],
   "source": [
    "#Define a variable for input dimension\n",
    "input_dim = 2\n",
    "\n",
    "#Define a variable for output dimension\n",
    "out_dim = 1\n",
    "\n",
    "#Define a variable for voltage decay\n",
    "vdecay=0.3\n",
    "\n",
    "#Define a variable for voltage threshold\n",
    "vth=0.2\n",
    "\n",
    "#Define a variable for snn timesteps\n",
    "snn_timestep=10\n",
    "\n",
    "#Initialize randomly the weights from input to output. Numpy random rand function might be useful here. \n",
    "_weight = np.random.rand(out_dim, input_dim)\n",
    "input_2_output_weight = copy.copy(_weight)\n",
    "#print the initial weights\n",
    "print(\"Initial weights:\",input_2_output_weight)\n",
    "\n",
    "#Initialize an snn using the arguments defined above\n",
    "snn = SNN(input_2_output_weight, input_dim, out_dim, vdecay, vth, snn_timestep)\n",
    "#Get the training data for AND gate using the function defined in 2a. \n",
    "train_data = genANDTrainData(snn_timestep)\n",
    "#Train the network using the function defined in 2c. with the appropriate arguments\n",
    "hebbian(snn, train_data)\n",
    "\n",
    "pred=[]\n",
    "#Test the trained network and print the network output for all 4 cases. \n",
    "for x in range(len(train_data)):\n",
    "    print(\"Interation \"+str(x)+\":\")\n",
    "    print(train_data[x][0])\n",
    "    out = snn(train_data[x][0])\n",
    "    if out[0] >= 0.6:\n",
    "        pred.append(1)\n",
    "    else:\n",
    "        pred.append(0)\n",
    "    print(\"Output Spiking Rate:\",str(out))\n",
    "print(\"\\nPrediction: \", str(pred))\n",
    "    \n",
    "#Print Final Network Weights\n",
    "print(\"Final weights = \", str(snn.input_2_output_connection.weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Limitations of Hebbian Learning rule\n",
    "\n",
    "## 3a. \n",
    "Can you learn the AND gate using 2 neurons in the output layer instead of one? If yes, describe what changes you might need to make to your algorithm in 2b. If not, explain why not, and what consequences it might entail for the use of hebbian learning for complex real-world tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 3a. \n",
    "When the output layer has two neurons, it can represent the absence or presence of each potential output (either 0 or 1) for the AND gate. However, this interpretation doesn't match well with the fundamental Hebbian learning rule, which doesn't optimize for probabilities but instead models the correlation between input and output activity.\n",
    "\n",
    "The Hebbian learning rule has limitations because it can't learn negative weights, resulting in weight saturation and reduced selectivity. In the case of a two-neuron output layer, this restriction implies that the input-output neuron connections will grow and eventually saturate, making it difficult for the network to learn the AND gate correctly.\n",
    "\n",
    "If we want to use two output neurons for the AND gate, we must adjust the weights and connections accordingly. Specifically, we would need four weights connecting each input neuron to each output neuron, and we would need to establish connections between the existing neurons and the additional neuron in the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. \n",
    "Train the network using hebbian learning for AND gate with the same arguments as defined in 2d. but now multiply the number of epochs by 20. Can your network still learn AND gate correctly? Inspect the initial and final network weights, and compare them against the network weights in 2d. Based on this, explain your observations for the network behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights =  [[0.13681335 0.17209685]]\n",
      "0:\n",
      "[[0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0]]\n",
      "Output Spiking Rate: [0.25]\n",
      "1:\n",
      "[[0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0]\n",
      " [1 0 0 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1]]\n",
      "Output Spiking Rate: [0.35]\n",
      "2:\n",
      "[[1 0 0 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1]\n",
      " [0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0]]\n",
      "Output Spiking Rate: [0.4]\n",
      "3:\n",
      "[[1 0 0 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1]\n",
      " [1 0 0 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1]]\n",
      "Output Spiking Rate: [0.65]\n",
      "\n",
      "Prediction:  [0, 0, 0, 1]\n",
      "Final weights =  [[0.42081335 0.45609685]]\n"
     ]
    }
   ],
   "source": [
    "#Implementation for 3b. (same as 2d. but with change of one argument)\n",
    "\n",
    "#Initialize randomly the weights from input to output. Numpy random rand function might be useful here.\n",
    "in_2_out_weight = np.random.rand(out_dim, input_dim)\n",
    "\n",
    "#print the initial weights\n",
    "print(\"Initial weights = \", str(in_2_out_weight))\n",
    "\n",
    "#Initialize an snn using the arguments defined above\n",
    "snn_n = SNN(in_2_out_weight, input_dim, out_dim, vdecay, vth, snn_timestep)\n",
    "\n",
    "#Get the training data for AND gate using the function defined in 2a. \n",
    "train_data = genANDTrainData(snn_timestep)\n",
    "\n",
    "#Train the network using the function defined in 2c. with the appropriate arguments\n",
    "hebbian(snn_n, train_data, lr=1e-3, epochs=10*20)\n",
    "\n",
    "pred_n = []\n",
    "#Test the trained network and print the network output for all 4 cases.\n",
    "for ii in range(len(train_data)):\n",
    "    print(str(ii)+\":\") \n",
    "    print(train_data[ii][0])\n",
    "    out = snn_n(train_data[ii][0])\n",
    "    if out[0] >= 0.6:\n",
    "        pred_n.append(1)\n",
    "    else:\n",
    "        pred_n.append(0)\n",
    "    print(\"Output Spiking Rate:\", str(out))\n",
    "print(\"\\nPrediction: \", str(pred_n))\n",
    "\n",
    "#Print Final Network Weights\n",
    "print(\"Final weights = \", str(snn_n.input_2_output_connection.weights))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 3b. \n",
    "Increasing the number of epochs to 20 times the previous attempt and training the network using the Hebbian learning rule results in an incorrect learning of the AND gate. The weights of the network keep increasing as the epochs increase, causing more spiking activity and incorrect classification of inputs.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3c. \n",
    "Based on your observations and response in 3b., can you explain another limitation of hebbian learning rule w.r.t. weight growth? Can you also suggest a possible remedy for it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 3c. \n",
    "The Hebbian learning rule has a limitation in that there is no mechanism for weight decay, and weights can grow uncontrollably. Eventually, the weights will saturate in relation to the membrane properties of LIF, and the network may not be as effective at learning new information. To overcome this, one possible solution is to use Oja's rule, which asymptotically converges to synaptic weights that are normalized to $\\sum_{j} w_{ij}^{2}=1$. This normalization ensures that if some weights increase, others must decrease, preventing saturation of weights.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3d. \n",
    "To resolve the issues with hebbian learning, one possibility is Oja's rule. In this exercise, you will implement and train an SNN using Oja's learning rule. \n",
    "\n",
    "References: https://neuronaldynamics.epfl.ch/online/Ch19.S2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oja(network, train_data, lr=1e-5, epochs=10):\n",
    "    \"\"\" \n",
    "    Function to train a network using Hebbian learning rule\n",
    "        Args:\n",
    "            network (SNN): SNN network object\n",
    "            train_data (list): training data \n",
    "            lr (float): learning rate\n",
    "            epochs (int): number of epochs to train with. Each epoch is defined as one pass over all training samples. \n",
    "        \n",
    "        Write the operations required to compute the weight increment according to the hebbian learning rule. Then increment the network weights. \n",
    "    \"\"\"\n",
    "    \n",
    "    #iterate over the epochs\n",
    "    for ee in range(epochs):\n",
    "        #iterate over all samples in train_data\n",
    "        for data in train_data:\n",
    "            #compute the firing rate for the input\n",
    "            oja_v_i = np.sum(data[0], axis=1)/len(data[1])\n",
    "            #compute the firing rate for the output\n",
    "            oja_v_j = np.sum(data[1])/len(data[1])\n",
    "            #compute the weight increment\n",
    "            oja_delta_w = lr * oja_v_j * (oja_v_i - (network.input_2_output_connection.weights * oja_v_j))\n",
    "            #increment the weight\n",
    "            network.input_2_output_connection.weights += oja_delta_w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, test your implementation below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights =  [[0.26998317 0.96383494]]\n",
      "0:\n",
      "[[1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0]\n",
      " [1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0]]\n",
      "Output Spiking Rate: [0.3]\n",
      "1:\n",
      "[[1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0]\n",
      " [1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1]]\n",
      "Output Spiking Rate: [0.8]\n",
      "2:\n",
      "[[1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1]\n",
      " [1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0]]\n",
      "Output Spiking Rate: [0.35]\n",
      "3:\n",
      "[[1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1]\n",
      " [1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1]]\n",
      "Output Spiking Rate: [0.8]\n",
      "\n",
      "Prediction:  [0, 1, 0, 1]\n",
      "Final weights =  [[0.2700646  0.96385323]]\n"
     ]
    }
   ],
   "source": [
    "#Define a variable for input dimension\n",
    "oja_input_dimension = 2\n",
    "#Define a variable for output dimension\n",
    "oja_output_dimension = 1\n",
    "#Define a variable for voltage decay\n",
    "vdecay = 0.5\n",
    "#Define a variable for voltage threshold\n",
    "vth = 0.5\n",
    "#Define a variable for snn timesteps\n",
    "snn_timestep = 20\n",
    "#Initialize randomly the weights from input to output. Numpy random rand function might be useful here. \n",
    "oja_input_2_output_weight = np.random.rand(oja_output_dimension, oja_input_dimension)\n",
    "#print the initial weights\n",
    "print(\"Initial weights = \", oja_input_2_output_weight)\n",
    "\n",
    "#Initialize an snn using the arguments defined above\n",
    "oja_snn = SNN(oja_input_2_output_weight, oja_input_dimension, oja_output_dimension, vdecay, vth, snn_timestep)\n",
    "#Get the training data for AND gate using the function defined in 2a. \n",
    "oja_train_data = genANDTrainData(snn_timestep)\n",
    "#Train the network using the function defined in 3d. with the appropriate arguments\n",
    "oja(oja_snn, oja_train_data)\n",
    "\n",
    "oja_pred =[]\n",
    "#Test the trained network and print the network output for all 4 cases. \n",
    "for x in range(len(oja_train_data)):\n",
    "    print(str(x)+\":\") \n",
    "    print(oja_train_data[x][0])\n",
    "    oja_out = oja_snn(oja_train_data[x][0])\n",
    "    if oja_out[0] >= 0.6:\n",
    "        oja_pred.append(1)\n",
    "    else:\n",
    "        oja_pred.append(0)\n",
    "    print(\"Output Spiking Rate:\",str(oja_out))\n",
    "print(\"\\nPrediction: \", str(oja_pred))\n",
    "\n",
    "#Print Final Network Weights\n",
    "print(\"Final weights = \", str(oja_snn.input_2_output_connection.weights))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Spike-time dependent plasticity (STDP)\n",
    "\n",
    "Reference: https://neuronaldynamics.epfl.ch/online/Ch19.S5.html\n",
    "\n",
    "## 4a. \n",
    "What is the limitation with hebbian learning that STDP aims to resolve?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 4a. \n",
    "Hebbian learning is based on the principle of synaptic plasticity. Hebb's rule considers correlation between the pre and post synaptic firing rates. However, it's limitation is that it does not take into account the timing of the pre and post dynaptic spikes, which can lead to inefficient learning conditions. Whereas, STDP takes into account the precise timing of the spikes in the pre and post synaptic neurons. A synapse is strengthened between two neurons when th epre-synaptic neuron fires hortly before the post-synaptic neuron, and weakens the synapse when the post-synaptic neuron fires shortly before the pre-synaptic neuron. STDP allows the network to learn more precisely when to activate specific synapses. Another drawback of hebbian learning is that the networks trained on it forget the previously learned information when learning new information. STDP addresses this by allowing the network to selectively strengthen or weaken specific synapses based on their activity, rather than overwriting all of the synapses in the network during each learning cycle.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. \n",
    "Describe the algorithm to train a network using STDP learning rule. You do not need to describe encoding here. Your algorithm should be such that its naturally translatable to a program. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 4b. \n",
    "- STDP's main idea is that it is the timing of spikes rather than the specific rate of spikes that carries neural information.\n",
    "> - Each spike leaves a trace.\n",
    "> - At the moment of a presynaptic spike, a decrease of the weight is induced proportional to the value of the postsynaptic trace.\n",
    "> - Analogously, at the moment of a postsynaptic spike, potentiation of the weight occurs, which is proportional to the presynaptic trace left by a previous presynaptic spike\n",
    "<br><br>\n",
    ">   - Initailise the network with random weights.\n",
    ">   - If a presynaptic spike takes place right before postsynaptic spike, the presynaptic trace at the time of postsynaptic spike is high. So, weight increase will be high. (corresponds to $\\Delta t \\ge 0$)\n",
    ">   - If a presynaptic spike takes place right after postsynaptic spike, the presynaptic trace plays no role in the postsynaptic spike. So, weight decrease will be high (corresponds to $\\Delta t < 0$)\n",
    ">   - Below equation indicates the respective increase and decrease in weights in accordance with spike trace:\n",
    "<br>$\\Delta w = A$<sup>+</sup>$exp(-\\Delta t/\\tau$<sub>p</sub>) &nbsp;&nbsp;$if \\Delta t \\ge 0$\n",
    "<br>$\\Delta w = -A$<sup>-</sup>$exp(-\\Delta t/\\tau$<sub>m</sub>) &nbsp;&nbsp;$if \\Delta t < 0$ \n",
    ">   - Update the weights in accordance with $\\Delta w$:\n",
    "<br>w<sub>new</sub> = w<sub>old</sub> + $\\alpha \\Delta w$(w<sub>max</sub> - w<sub>old</sub>), &nbsp;&nbsp; $if \\Delta w > 0$\n",
    "<br>w<sub>new</sub> = w<sub>old</sub> + $\\alpha \\Delta w$(w<sub>old</sub> - w<sub>min</sub>), &nbsp;&nbsp; $if \\Delta w \\le 0$\n",
    ">   - Repeat for specified number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4c. \n",
    "In this exercise, you will implement the STDP learning algorithm to train a network. STDP has many different flavors. For this exercise, we will use the learning rule defined in: https://dl.acm.org/doi/pdf/10.1609/aaai.v33i01.330110021. Pay special attention to Equations 2 and 3. \n",
    "\n",
    "Below is the class definition for STDP learning algorithm. Your task is to fill in the components so that the weights are updated in the right manner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STDP():\n",
    "    \"\"\"Train a network using STDP learning rule\"\"\"\n",
    "    def __init__(self, network, A_plus, A_minus, tau_plus, tau_minus, lr, snn_timesteps=20, epochs=30, w_min=0, w_max=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            network (SNN): network which needs to be trained\n",
    "            A_plus (float): STDP hyperparameter\n",
    "            A_minus (float): STDP hyperparameter\n",
    "            tau_plus (float): STDP hyperparameter\n",
    "            tau_minus (float): STDP hyperparameter\n",
    "            lr (float): learning rate\n",
    "            snn_timesteps (int): SNN simulation timesteps\n",
    "            epochs (int): number of epochs to train with. Each epoch is defined as one pass over all training samples.  \n",
    "            w_min (float): lower bound for the weights\n",
    "            w_max (float): upper bound for the weights\n",
    "        \"\"\"\n",
    "        self.network = network\n",
    "        self.A_plus = A_plus\n",
    "        self.A_minus = A_minus\n",
    "        self.tau_plus = tau_plus\n",
    "        self.tau_minus = tau_minus\n",
    "        self.snn_timesteps = snn_timesteps\n",
    "        self.lr = lr\n",
    "        self.time = np.arange(0, self.snn_timesteps, 1)\n",
    "        self.sliding_window = np.arange(-4, 4, 1) #defines a sliding window for STDP operation. \n",
    "        self.epochs = epochs\n",
    "        self.w_min = w_min\n",
    "        self.w_max = w_max\n",
    "    \n",
    "    def update_weights(self, t, i):\n",
    "        \"\"\"\n",
    "        Function to update the network weights using STDP learning rule\n",
    "        \n",
    "        Args:\n",
    "            t (int): time difference between postsynaptic spike and a presynaptic spike in a sliding window\n",
    "            i(int): index of the presynaptic neuron\n",
    "        \n",
    "        Fill the details of STDP implementation\n",
    "        \"\"\"\n",
    "        #compute delta_w for positive time difference\n",
    "        if t>0:\n",
    "            delta_w = self.A_plus * math.exp((-1)*t/self.tau_plus)\n",
    "\n",
    "        #compute delta_w for negative time difference\n",
    "        else:\n",
    "            delta_w = -self.A_minus * math.exp((-1)*t/self.tau_minus)\n",
    "\n",
    "            \n",
    "        #update the network weights if weight increment is negative\n",
    "        if delta_w < 0:\n",
    "            self.network.input_2_output_connection.weights[:,i] += self.lr * delta_w *(self.network.input_2_output_connection.weights[:,i] - self.w_min)\n",
    "\n",
    "        #update the network weights if weight increment is positive\n",
    "        elif delta_w > 0:\n",
    "            self.network.input_2_output_connection.weights[:,i] += self.lr * delta_w *(self.w_max - self.network.input_2_output_connection.weights[:,i])\n",
    "\n",
    "\n",
    "            \n",
    "    def train_step(self, train_data_sample):\n",
    "        \"\"\"\n",
    "        Function to train the network for one training sample using the update function defined above. \n",
    "        \n",
    "        Args:\n",
    "            train_data_sample (list): a sample from the training data\n",
    "            \n",
    "        This function is complete. You do not need to do anything here. \n",
    "        \"\"\"\n",
    "        input = train_data_sample[0]\n",
    "        output = train_data_sample[1]\n",
    "        for t in self.time:\n",
    "            if output[t] == 1:\n",
    "                for i in range(2):\n",
    "                    for t1 in self.sliding_window:\n",
    "                        if (0<= t + t1 < self.snn_timesteps) and (t1!=0) and (input[i][t+t1] == 1):\n",
    "                            self.update_weights(t1, i)\n",
    "    \n",
    "    def train(self, training_data):\n",
    "        \"\"\"\n",
    "        Function to train the network\n",
    "        \n",
    "        Args:\n",
    "            training_data (list): training data\n",
    "        \n",
    "        This function is complete. You do not need to do anything here. \n",
    "        \"\"\"\n",
    "        for ee in range(self.epochs):\n",
    "            for train_data_sample in training_data:\n",
    "                self.train_step(train_data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Weights =  [[0.88287353 0.17866873]]\n",
      "0:\n",
      "[[0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1]\n",
      " [0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1]]\n",
      "Output Spiking Rate: [0.25]\n",
      "1:\n",
      "[[0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1]\n",
      " [0 0 1 1 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1]]\n",
      "Output Spiking Rate: [0.25]\n",
      "2:\n",
      "[[0 0 1 1 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1]\n",
      " [0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1]]\n",
      "Output Spiking Rate: [0.6]\n",
      "3:\n",
      "[[0 0 1 1 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1]\n",
      " [0 0 1 1 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1]]\n",
      "Output Spiking Rate: [0.6]\n",
      "\n",
      "Prediction:  [0, 0, 1, 1]\n",
      "Final weights =  [[0.82019287 0.16789911]]\n"
     ]
    }
   ],
   "source": [
    "#Define a variable for input dimension\n",
    "input_dimension = 2\n",
    "\n",
    "#Define a variable for output dimension\n",
    "output_dimension =1\n",
    "\n",
    "#Define a variable for voltage decay\n",
    "vdecay = 0.5\n",
    "\n",
    "#Define a variable for voltage threshold\n",
    "vth = 0.5\n",
    "\n",
    "#Define a variable for snn timesteps\n",
    "snn_timestep = 20\n",
    "\n",
    "#Initialize randomly the weights from input to output. Numpy random rand function might be useful here.\n",
    "ip_2_op_weight = np.random.rand(output_dimension, input_dimension) \n",
    "print(\"Initial Weights = \", str(ip_2_op_weight))\n",
    "\n",
    "#Initialize an snn using the arguments defined above\n",
    "snn_stdp = SNN(ip_2_op_weight, input_dimension, output_dimension, vdecay, vth, snn_timestep)\n",
    "\n",
    "#Get the training data for AND gate using the function defined in 2a.\n",
    "hebbian_train_data = genANDTrainData(snn_timestep)\n",
    "\n",
    "#Create an object of STDP class with appropriate arguments\n",
    "stdp = STDP(snn_stdp, 0.8, 0.2, 3, 0.8, 1e-5, snn_timestep, epochs=20)\n",
    "\n",
    "#Train the network using STDP\n",
    "stdp.train(hebbian_train_data)\n",
    "\n",
    "stdp_pred = []\n",
    "#Test the trained network and print the network output for all 4 cases. \n",
    "for ii in range(len(hebbian_train_data)):\n",
    "    print(str(ii)+\":\") \n",
    "    print(hebbian_train_data[ii][0])\n",
    "    stdp_out = snn_stdp(hebbian_train_data[ii][0])\n",
    "    if stdp_out[0] >= 0.6:\n",
    "        stdp_pred.append(1)\n",
    "    else:\n",
    "        stdp_pred.append(0)\n",
    "    print(\"Output Spiking Rate:\",str(stdp_out))\n",
    "print(\"\\nPrediction: \", str(stdp_pred))\n",
    "\n",
    "print(\"Final weights = \", str(snn_stdp.input_2_output_connection.weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: OR Gate\n",
    "Can you train the network with the same architecture in Q2-4 for learning the OR gate. You will need to create another function called genORTrainData. Then create an SNN and train it using STDP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your implementation of genORTrainData here. \n",
    "\n",
    "def genORTrainData(snn_timestep):\n",
    "    \"\"\" \n",
    "    Function to generate the training data for OR \n",
    "        Args:\n",
    "            snn_timestep (int): timesteps for SNN simulation\n",
    "        Return:\n",
    "            train_data (list): list of tuples where each tuple comprises of numpy arrays of rate-coded inputs and output\n",
    "        \n",
    "        Write the expressions for encoding 0 and 1. Then append all 4 cases of OR gate to the list train_data\n",
    "    \"\"\"\n",
    "    \n",
    "    #Initialize an empty list for train data\n",
    "    train_data = []\n",
    "    \n",
    "    #encode 0\n",
    "    x1 = np.random.choice([0,1], snn_timestep, True, [0.7,0.3])\n",
    "    \n",
    "    #encode 1\n",
    "    x2 = np.random.choice([0,1], snn_timestep, True, [0.3,0.7])\n",
    "    \n",
    "    #Append all 4 cases of OR gate to train_data\n",
    "    cs_00 = [np.array([x1,x1]),x1]\n",
    "    cs_01 = [np.array([x1,x2]),x2]\n",
    "    cs_10 = [np.array([x2,x1]),x2]\n",
    "    cs_11 = [np.array([x2,x2]),x2]\n",
    "    train_data = np.stack((cs_00, cs_01, cs_10, cs_11))\n",
    "\n",
    "    return train_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Weights =  [[0.30869925 0.83766268]]\n",
      "0:\n",
      "[[1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0]\n",
      " [1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0]]\n",
      "Output Spiking Rate: [0.2]\n",
      "1:\n",
      "[[1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0]\n",
      " [0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1]]\n",
      "Output Spiking Rate: [0.8]\n",
      "2:\n",
      "[[0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1]\n",
      " [1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0]]\n",
      "Output Spiking Rate: [0.2]\n",
      "3:\n",
      "[[0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1]\n",
      " [0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1]]\n",
      "Output Spiking Rate: [0.8]\n",
      "\n",
      "Prediction:  [0, 1, 0, 1]\n",
      "Final weights =  [[0.23484473 0.62648191]]\n"
     ]
    }
   ],
   "source": [
    "#Train the network for OR gate here using the implementation from 4c. \n",
    "\n",
    "#Initialize randomly the weights from input to output. Numpy random rand function might be useful here.\n",
    "ip_2_op_weight = np.random.rand(output_dimension, input_dimension) \n",
    "print(\"Initial Weights = \", str(ip_2_op_weight))\n",
    "\n",
    "#Initialize an snn using the arguments defined above\n",
    "snn_OR_stdp = SNN(ip_2_op_weight, input_dimension, output_dimension, vdecay, vth, snn_timestep)\n",
    "\n",
    "#Get the training data for AND gate using the function defined in 2a.\n",
    "hebbian_OR_data = genORTrainData(snn_timestep)\n",
    "\n",
    "#Create an object of STDP class with appropriate arguments\n",
    "OR_stdp = STDP(snn_OR_stdp, 0.8, 0.3, 2, 0.9, 1e-5, snn_timestep, epochs=30)\n",
    "\n",
    "#Train the network using STDP\n",
    "OR_stdp.train(hebbian_OR_data)\n",
    "\n",
    "stdp_ORpred = []\n",
    "#Test the trained network and print the network output for all 4 cases. \n",
    "for ii in range(len(hebbian_OR_data)):\n",
    "    print(str(ii)+\":\") \n",
    "    print(hebbian_OR_data[ii][0])\n",
    "    stdp_ORout = snn_OR_stdp(hebbian_OR_data[ii][0])\n",
    "    if stdp_ORout[0] >= 0.6:\n",
    "        stdp_ORpred.append(1)\n",
    "    else:\n",
    "        stdp_ORpred.append(0)\n",
    "    print(\"Output Spiking Rate:\",str(stdp_ORout))\n",
    "print(\"\\nPrediction: \", str(stdp_ORpred))\n",
    "\n",
    "print(\"Final weights = \", str(snn_OR_stdp.input_2_output_connection.weights))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
