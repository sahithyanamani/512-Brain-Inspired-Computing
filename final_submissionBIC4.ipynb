{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Goals\n",
    "The goal of this assignment is to use the knowledge gained in the course to develop an end-to-end SNN for MNIST digits classification trained using state-of-the-art gradient-descent algorithm. Along the way, you will also learn the basics of developing any machine learning application- how to handle data using data loaders; defining and optimizing loss; evaluating an algorithm using validation set; speeding up training using GPUs. You will also learn the basics of programming using PyTorch which is by far the most widely used library for machine learning research- being used in applications such as autonomous driving, robotic control, cancer research, and much more!\n",
    "\n",
    "Let's import all the libraries required for this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, sampler\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Limitation of backprop for SNN\n",
    "\n",
    "## 1a. \n",
    "Sketch out the algorithm for training an SNN using backpropagation. Your algorithm should describe when and how the weights are updated. What is the main limitation of using backpropagation for training an SNN? Describe a solution to resolve the limitation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1a. \n",
    "Backpropagation is used to adjust the weight of the SNN. This works in the following way.\n",
    "1. The randomly initiated weight will give us some output. \n",
    "2. Once the output is recieved, we compare it with the actual value and calculate the loss. \n",
    "3. After calculating the loss, we compute the gradient. \n",
    "4. After finding the gradient we multiply it by an amplifying factor. \n",
    "5. This value is then back propogated i.e, each weight is multiplied by this gradient value. \n",
    "6. This process keeps happening till the minima of the gradient is attained (which is the point at which loss is minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. \n",
    "In this exercise, we will implement the solution for overcoming backprop limitation for SNN. First the preliminaries: In PyTorch, arrays are called tensors. Gradients are computed automatically using the automatic differentiation package (autograd). The main elements of an autograd function are the forward and backward functions. The forward function simply performs the forward pass, i.e. computing output tensors from the input tensors. The backward function receives the gradient of the output tensors w.r.t. some scalar value, and computes the gradient of the input tensors w.r.t. the same scalar value. \n",
    "\n",
    "Below, we define the autograd function class for pseudo-gradient using the rectangular function. Most of the implementation is already written for you. Your task is to fill two key components- i) in the forward function, write the implementation for generating spike outputs from the inputs; ii) in the backward function, write the implementation for computing the gradient of the spike using rectangular psuedo-grad function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoSpikeRect(torch.autograd.Function):\n",
    "    \"\"\" Rectangular Pseudo-grad function \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, vth, grad_win, grad_amp):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (Torch Tensor): Input tensor containing voltages of neurons in a layer\n",
    "            vth (Float): Voltage threshold for spiking \n",
    "            grad_win (Float): Window for computing pseudogradient\n",
    "            grad_amp (Float): Amplification factor for the gradients\n",
    "        \n",
    "        Returns:\n",
    "            output (Torch Tensor): Generated spikes for the input\n",
    "        \n",
    "        Write the operation for computing the output spikes from the input. The operation should be vectorized, i.e. no loops. \n",
    "        \"\"\"\n",
    "        \n",
    "        #Saving variables for backward pass. Nothing to do here\n",
    "        ctx.save_for_backward(input)\n",
    "        ctx.vth = vth\n",
    "        ctx.grad_win = grad_win\n",
    "        ctx.grad_amp = grad_amp\n",
    "        \n",
    "        #Compute output from the input. No loops. Hint: Use Pytorch \"greater than\" function. \n",
    "        output = torch.gt(input,ctx.vth).float()\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            grad_output (Torch Tensor): Gradient of the output\n",
    "        \n",
    "        Returns:\n",
    "            grad (Torch Tensor): Gradient of the input\n",
    "        \n",
    "        Write the operation for computing the output spikes from the input. The operation should be vectorized, i.e. no loops. \n",
    "        \"\"\"\n",
    "        \n",
    "        #Retrieving variables from forward pass. Nothing to do here. \n",
    "        input, = ctx.saved_tensors\n",
    "        vth = ctx.vth\n",
    "        grad_win = ctx.grad_win\n",
    "        grad_amp = ctx.grad_amp\n",
    "        grad_input = grad_output.clone()\n",
    "        \n",
    "        #compute the gradient of the input using rectangular pseudograd function\n",
    "        spike_pseudo_grad = abs(input - vth) < grad_win \n",
    "        \n",
    "        #Multiplying by gradient amplifier. Nothing to do here\n",
    "        grad = grad_amp * grad_input * spike_pseudo_grad.float()\n",
    "        return grad, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Creating a Layer\n",
    "\n",
    "In this exercise, we will create the class definition for a layer of LIF neurons (similar to the implementation in Assignment 2).\n",
    "\n",
    "Below is the class definition of an LIF neuron layer. Your task is to write the operation for integrating the presynaptic spikes into voltage, and then transforming to spikes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearIFCell(nn.Module):\n",
    "    \"\"\" Leaky Integrate-and-fire neuron layer\"\"\"\n",
    "\n",
    "    def __init__(self, psp_func, pseudo_grad_ops, param):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            psp_func (Torch Function): Pre-synaptic function\n",
    "            pseudo_grad_ops (Torch Function): Pseudo-grad function\n",
    "            param (tuple): Cell parameters (Voltage Threshold, gradient window, gradient amplitude)\n",
    "        \n",
    "        This function is complete. You do not need to do anything here. \n",
    "        \"\"\"\n",
    "        super(LinearIFCell, self).__init__()\n",
    "        self.psp_func = psp_func\n",
    "        self.pseudo_grad_ops = pseudo_grad_ops\n",
    "        self.vdecay, self.vth, self.grad_win, self.grad_amp = param\n",
    "\n",
    "    def forward(self, input_data, state):\n",
    "        \"\"\"\n",
    "        Forward function\n",
    "        Args:\n",
    "            input_data (Tensor): input spike from pre-synaptic neurons\n",
    "            state (tuple): output spike of last timestep and voltage of last timestep\n",
    "        Returns:\n",
    "            output: output spike\n",
    "            state: updated neuron states\n",
    "        \n",
    "        Write the operation for integrating the presynaptic spikes into voltage.\n",
    "        \"\"\"\n",
    "        pre_spike, pre_volt = state\n",
    "        \n",
    "        #Compute the voltage from the presynaptic inputs. This should be a vectorized operation. No loops. \n",
    "        #volt = torch.dot(input_data,state)\n",
    "        \n",
    "        volt = (self.vdecay * pre_volt*(1 - pre_spike))\n",
    "        volt= volt + self.psp_func(input_data)\n",
    "        \n",
    "        #Compute the spike output by using the pseudo_grad_ops function. This should be a vectorized operation. No loops.\n",
    "        #output = torch.dot(pseudo_grad_ops(),volt)\n",
    "        output = self.pseudo_grad_ops(volt, self.vth, self.grad_win, self.grad_amp)\n",
    "        \n",
    "        return output, (output, volt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Creating a Network \n",
    "\n",
    "## 3a.\n",
    "We will now create a one-layer SNN using the class definitions above. Preliminaries: In Assignment 2, the psp was computed using numpy matrix multiplication of weights and inputs. In PyTorch, nn.Linear() achieves the same. You can find the documentation here: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html. We will use this class to serve as our psp function required for creating a layer according to the implementation in Q2. \n",
    "\n",
    "Below is the class definition of a network. Your task is to fill in the required components in the init and forward functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHiddenLayerSNN(nn.Module):\n",
    "    \"\"\" SNN with single hidden layer \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, param_dict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): input dimension\n",
    "            output_dim (int): output dimension\n",
    "            hidden_dim (int): hidden layer dimension\n",
    "            param_dict (dict): neuron parameter dictionary for each layer (Voltage Threshold, gradient window, gradient amplitude)\n",
    "        \n",
    "        Create hidden and output layers using implementation of the layer in Q2. and using nn.Linear as the psp function. \n",
    "        \"\"\"\n",
    "        super(SingleHiddenLayerSNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        pseudo_grad_ops = PseudoSpikeRect.apply\n",
    "        \n",
    "        #Create the hidden layer. Assume that the hidden layer neuron parameters are in param_dict['hid_layer']. Set bias=False for nn.Linear.  \n",
    "        self.hidden_cell = LinearIFCell((nn.Linear(self.input_dim,self.hidden_dim,bias=False)),pseudo_grad_ops,param_dict['hid_layer'])\n",
    "        \n",
    "        \n",
    "        #Create the output layer. Output layer params are in param_dict['out_layer']. Set bias=False for nn.Linear.   \n",
    "        self.output_cell = LinearIFCell((nn.Linear(self.hidden_dim, self.output_dim,bias=False)), pseudo_grad_ops,param_dict['out_layer'])\n",
    "        \n",
    "\n",
    "    def forward(self, spike_data, init_states_dict, batch_size, spike_ts):\n",
    "        \"\"\"\n",
    "        Forward function\n",
    "        Args:\n",
    "            spike_data (Tensor): spike data input (batch_size, input_dim, spike_ts)\n",
    "            init_states_dict (dict): initial states for each layer- 'hid_layer' for hidden layer; 'out_layer' for output layer. \n",
    "            batch_size (int): batch size\n",
    "            spike_ts (int): spike timesteps\n",
    "        Returns:\n",
    "            output: number of spikes of output layer\n",
    "        \n",
    "        Write the operations for propagating the input through the network and computing the spike outputs. \n",
    "        \"\"\"\n",
    "        hidden_state, out_state = init_states_dict['hid_layer'], init_states_dict['out_layer']\n",
    "        spike_data_flatten = spike_data.view(batch_size, self.input_dim, spike_ts)\n",
    "        output_list = [] #List to store the output at each timestep\n",
    "        for tt in range(spike_ts):\n",
    "            #Retrieve the input at time tt\n",
    "            input_tt = spike_data_flatten[:,:,tt]\n",
    "            \n",
    "            #Propagate through the hidden layer\n",
    "            hidden_output,hidden_state = self.hidden_cell(input_tt,hidden_state)\n",
    "            \n",
    "            #Propagate through the output layer\n",
    "            output_temp, out_state = self.output_cell(hidden_output,out_state)\n",
    "            \n",
    "            \n",
    "            #Append output spikes to output list\n",
    "            output_list.append(output_temp[:])\n",
    "        \n",
    "        #Sum the outputs to compute spike count for each output neuron. Torch.stack and Torch.sum might be useful here. No loops\n",
    "        tmp=torch.stack(output_list)\n",
    "        #output = torch.sum(torch.stack(output_list),1)\n",
    "        output = torch.sum(tmp,axis=0)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. \n",
    "Next, we need a Wrapper class that: i) Initializes the parameters required for creating the SNN class object; ii) creates the SNN class objects using the initial parameters; iii) Computes the SNN output and returns it. The class is already written for you. You do not need to do anything here. Just understand the implementation so that you can use it later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapSNN(nn.Module):\n",
    "    \"\"\" Wrapper of SNN \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, param_dict, device):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): input dimension\n",
    "            output_dim (int): output dimension\n",
    "            hidden_dim (int): hidden layer dimension\n",
    "            param_dict (dict): neuron parameter dictionary\n",
    "            device (device): device\n",
    "        \"\"\"\n",
    "        super(WrapSNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "        self.snn = SingleHiddenLayerSNN(input_dim, output_dim, hidden_dim, param_dict)\n",
    "\n",
    "    def forward(self, spike_data):\n",
    "        \"\"\"\n",
    "        Forward function\n",
    "        Args:\n",
    "            spike_data (Tensor): spike data input\n",
    "        Returns:\n",
    "            output: number of spikes of output layer\n",
    "        \"\"\"\n",
    "        batch_size = spike_data.shape[0]\n",
    "        spike_ts = spike_data.shape[-1]\n",
    "        init_states_dict = {}\n",
    "        # Hidden layer\n",
    "        hidden_volt = torch.zeros(batch_size, self.hidden_dim, device=self.device)\n",
    "        hidden_spike = torch.zeros(batch_size, self.hidden_dim, device=self.device)\n",
    "        init_states_dict['hid_layer'] = (hidden_spike, hidden_volt)\n",
    "        # Output layer\n",
    "        out_volt = torch.zeros(batch_size, self.output_dim, device=self.device)\n",
    "        out_spike = torch.zeros(batch_size, self.output_dim, device=self.device)\n",
    "        init_states_dict['out_layer'] = (out_spike, out_volt)\n",
    "        # SNN\n",
    "        output = self.snn(spike_data, init_states_dict, batch_size, spike_ts)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Encoding MNIST into spikes\n",
    "Following is the function that converts an MNIST image into spikes. You have already implemented it using Numpy in Assignment 2. The implementation remains the same- except that we will now use Torch tensors instead of numpy arrays. Fill in the components to convert a batch of torch tensors into spikes. Since the goal is to learn writing optimized code using PyTorch, you are supposed to do this without any loops. Use vector operations instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_2_event_img(image, device, spike_ts):\n",
    "    \"\"\"\n",
    "    Transform image to event image\n",
    "    Args:\n",
    "        image (Tensor): image\n",
    "        device (device): device (can be either CPU or GPU)\n",
    "        spike_ts (int): spike timestep\n",
    "    Returns:\n",
    "        event_image: event image\n",
    "    \"\"\"\n",
    "    batch_size = image.shape[0]\n",
    "    channel_size = image.shape[1]\n",
    "    image_size = image.shape[2]\n",
    "    image = image.view(batch_size, channel_size, image_size, image_size, 1)\n",
    "    \n",
    "    #Create a random image of shape batch_size x channel_size x image_size x image_size x spike_ts. Torch rand function might be useful here. \n",
    "    #Remember to put the random image on the device specified in the function argument. \n",
    "    random_image =torch.rand(batch_size,channel_size,image_size,image_size,spike_ts).to(device)\n",
    "    \n",
    "    #Generate event image using image and random image\n",
    "    event_image = torch.gt(image,random_image).float() \n",
    "\n",
    "    return event_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Training the SNN\n",
    "\n",
    "In this exercise, we will write the function to train an SNN using spatiotemporal backprop (stbp). A typical training loop works as follows:\n",
    "\n",
    "1. Split the dataset into train and test. The network is trained on all the batches in the train dataset, and then validated on the test dataset. This gives us an idea of how well the network generalizes to unseen data. \n",
    "\n",
    "2. A criterion is defined to compute the loss. For classification tasks, this is generally Cross Entropy (https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "\n",
    "3. The network is initialized with random weights. \n",
    "\n",
    "4. Typically, the training is done on mini-batches of train data. This means that at every training instance, the network receives batches of training data where each batch contains small number of samples. The loss and the gradients required for updating the weights are averaged over these batches. \n",
    "\n",
    "5. Within every training iteration, first we retrieve the batches of data. We compute the network prediction. Then we compute the loss by comparing the network prediction against the true labels. Then the gradient of the loss with respect to all the network weights is computed. This gradient is then used to update the weights in the network.  \n",
    "\n",
    "Thankfully, PyTorch provides APIs to automate most of the above steps. Below is the function training an SNN that implements the algorithm presented above using PyTorch. Your task is to fill the components. Refer to the comments for hints on what PyTorch functions to use.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stbp_snn_training(network, spike_ts, device, batch_size=128, test_batch_size=256, epoch=100):\n",
    "    \"\"\"\n",
    "    STBP SNN training\n",
    "    Args:\n",
    "        network (SNN): STBP learning SNN\n",
    "        spike_ts (int): spike timestep\n",
    "        device (device): device\n",
    "        batch_size (int): batch size for training\n",
    "        test_batch_size (int): batch size for testing\n",
    "        epoch (int): number of epochs\n",
    "    Returns:\n",
    "        train_loss_list: list of training loss for each epoch\n",
    "        test_accuracy_list: list of test accuracy for each epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    #Creating folder where MNIST data is saved. Loading the MNIST dataset. This code is complete. Do not touch it. \n",
    "    try:\n",
    "        os.mkdir(\"./data\")\n",
    "        print(\"Directory data Created\")\n",
    "    except FileExistsError:\n",
    "        print(\"Directory data already exists\")\n",
    "    \n",
    "    data_path = './data/'\n",
    "    train_dataset = torchvision.datasets.MNIST(root=data_path, train=True, download=True,\n",
    "                                               transform=transforms.ToTensor())\n",
    "    test_dataset = torchvision.datasets.MNIST(root=data_path, train=False, download=True,\n",
    "                                              transform=transforms.ToTensor())\n",
    "\n",
    "    # Train and test dataloader\n",
    "    \n",
    "    \n",
    "    #Given the train and test datasets, we need to create dataloaders to load the datasets in the right format. \n",
    "    #You can read about PyTorch Dataset and Dataloaders here: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "    #For now, this part is complete and you do not need to do anything here. \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                  shuffle=False, num_workers=4)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size,\n",
    "                                 shuffle=False, num_workers=4)\n",
    "\n",
    "    \n",
    "    # Next we need to define a criteria for computing the loss. \n",
    "    # Refer to https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html on how to define the cross entropy loss. \n",
    "    # Note that you just need to define the loss here (and not compute it) \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    #Define an optimizer that will perform the weight updates. You can find more about the optimizers in PyTorch here: https://pytorch.org/docs/stable/optim.html\n",
    "    #Taking the help of the documentation above, create an optimizer for stochastic gradient descent (SGD). \n",
    "    optimizer = optim.SGD(network.parameters(), lr=0.001, momentum=0.9)  #model kya hai , momentum=0.9?\n",
    "    \n",
    "\n",
    "    # List for saving loss and accuracy\n",
    "    train_loss_list, test_accuracy_list = [], []\n",
    "    test_num = len(test_dataset)\n",
    "\n",
    "    # Start training\n",
    "    \n",
    "    #Put the network on the device (typically GPU but can also be cpu)\n",
    "    network.to(device)\n",
    "    \n",
    "    #Loop for the epochs\n",
    "    for ee in range(epoch):\n",
    "        #Keep track of running loss\n",
    "        running_loss = 0.0\n",
    "        running_batch_num = 0\n",
    "        train_start = time.time()\n",
    "        \n",
    "        #Iterate over the training data in train dataloader\n",
    "        for data in train_dataloader:\n",
    "            \n",
    "            #Retrieve the image and label from data\n",
    "            image_train, label = data\n",
    "            \n",
    "            #Put the image and labels on the device\n",
    "            image_train.to(device)\n",
    "            label.to(device)\n",
    "            \n",
    "            #Convert images to event images\n",
    "            event_image_1 = img_2_event_img(image_train,device,spike_ts)\n",
    "            \n",
    "            #Before we backprop, we need to set the gradients for each tensor to zero. This is done using the zero_grad function in Pytorch\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Compute the network output for the event images\n",
    "            network_output = network(event_image_1)\n",
    "            \n",
    "            #Compute the loss using the criterion defined previously. Store in a variable called loss\n",
    "            loss = criterion(network_output,label)\n",
    "            \n",
    "            #Backpropagate the loss through the network. Use Pytorch backward() function: https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html\n",
    "            loss.backward()\n",
    "            \n",
    "            #Update the network weights by taking an optimizer 'step'. \n",
    "            #You can learn how to do that here: https://pytorch.org/docs/stable/optim.html#taking-an-optimization-step\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Updating tracking variables. Nothing to do here\n",
    "            running_loss += loss.item()\n",
    "            running_batch_num += 1\n",
    "        train_end = time.time()\n",
    "        train_loss_list.append(running_loss / running_batch_num)\n",
    "        print(\"Epoch %d Training Loss %.4f\" % (ee, train_loss_list[-1]), end=\" \")\n",
    "        \n",
    "        \n",
    "        #This ends one training iteration. After every training iteration, we can evaluate how well the network does on data that it has not seen before. \n",
    "        #This step is called testing and is done on test dataset. \n",
    "        \n",
    "        #Counter to keep track of the number of correct predictions\n",
    "        test_correct_num = 0\n",
    "        test_start = time.time()\n",
    "        with torch.no_grad():\n",
    "            for data in test_dataloader:\n",
    "                \n",
    "                #Retrieve the image and label from test data\n",
    "                image_test,label = data \n",
    "                \n",
    "                #Put the image and labels on the device\n",
    "                image_test.to(device)\n",
    "                label.to(device)\n",
    "                \n",
    "                #Convert the image into event images\n",
    "                event_image_2 = img_2_event_img(image_test, device, spike_ts)\n",
    "                \n",
    "                #Compute the network predictions and store in a variable called outputs\n",
    "                outputs = network(event_image_2)\n",
    "                \n",
    "                #Get the class label as the largest activation. This is complete. Nothing to do here.\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                #Compare the network predictions against the true labels and update the counter for correct predictions. No loops. \n",
    "                test_correct_num += (predicted == label).sum().item()\n",
    "        \n",
    "        #Updating tracking variables. Nothing to do here\n",
    "        test_end = time.time()\n",
    "        test_accuracy_list.append(test_correct_num / test_num)\n",
    "        print(\"Test Accuracy %.4f Training Time: %.1f Test Time: %.1f\" % (\n",
    "            test_accuracy_list[-1], train_end - train_start, test_end - test_start))\n",
    "    \n",
    "    #Return the loss and accuracies. Nothing to do here. \n",
    "    print(\"End Training\")\n",
    "    network.to('cpu')\n",
    "    return train_loss_list, test_accuracy_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything ready to train and test our SNN using backprop. All that is left to do is initialize the network with the right parameters and call the training function on it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory data already exists\n",
      "Epoch 0 Training Loss 0.7280 Test Accuracy 0.9223 Training Time: 93.3 Test Time: 9.5\n",
      "Epoch 1 Training Loss 0.1949 Test Accuracy 0.9470 Training Time: 98.0 Test Time: 10.2\n",
      "Epoch 2 Training Loss 0.1378 Test Accuracy 0.9567 Training Time: 56.3 Test Time: 5.8\n",
      "Epoch 3 Training Loss 0.1033 Test Accuracy 0.9644 Training Time: 53.4 Test Time: 6.0\n",
      "Epoch 4 Training Loss 0.0872 Test Accuracy 0.9658 Training Time: 53.8 Test Time: 6.2\n",
      "Epoch 5 Training Loss 0.0685 Test Accuracy 0.9625 Training Time: 50.3 Test Time: 6.6\n",
      "Epoch 6 Training Loss 0.0591 Test Accuracy 0.9664 Training Time: 63.2 Test Time: 7.2\n",
      "Epoch 7 Training Loss 0.0494 Test Accuracy 0.9688 Training Time: 63.5 Test Time: 5.5\n",
      "Epoch 8 Training Loss 0.0439 Test Accuracy 0.9690 Training Time: 50.3 Test Time: 7.3\n",
      "Epoch 9 Training Loss 0.0396 Test Accuracy 0.9694 Training Time: 62.5 Test Time: 7.3\n",
      "End Training\n"
     ]
    }
   ],
   "source": [
    "# Define the device on which training will be performed\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Define the input dimensions in a variable\n",
    "input_dim =784\n",
    "\n",
    "#Define the output dimensions in a variable\n",
    "output_dim = 10\n",
    "\n",
    "#Define the hidden layer dimension in a variable\n",
    "hidden_dim =196\n",
    "\n",
    "#Create a dictionary of the neuron parameters for the hidden and output layer. The keys should be 'hid_layer' and 'out_layer'.\n",
    "#The values of the dictionary should be a list of the neuron parameters for each layer where the list elements are [vdecay, vth, grad_win, grad_amp]\n",
    "param_dict={\n",
    "    \"hid_layer\": [0.5,0.5,0.3,2],\n",
    "    \"out_layer\": [0.5,0.5,0.3,2]\n",
    "}\n",
    "\n",
    "\n",
    "#Define snn timesteps in a variable\n",
    "snn_timesteps = 20\n",
    "\n",
    "#Create the SNN using the class definition in 3b and the arguments defined above\n",
    "snnObj = WrapSNN(input_dim, output_dim, hidden_dim, param_dict, device)\n",
    "\n",
    "#Define the following training parameters\n",
    "#Batch size for training\n",
    "batch_size = 128\n",
    "\n",
    "#Batch size for testing\n",
    "test_batch_size = 256\n",
    "\n",
    "#Epochs\n",
    "epochs = 10\n",
    "\n",
    "#Train the snn using the above arguments and the definition in Q5. \n",
    "train_loss_list,test_loss_list=stbp_snn_training(snnObj, snn_timesteps, device, batch_size, test_batch_size, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6: Concluding Remarks \n",
    "If you have been able to implement all the parts in Q.1.-Q.5., you can now feel proud of yourself for having learned the basics of a deep learning pipeline that powers all the modern AI applications. Of course, the exact implementation varies in the input and network design for more complex applications, but the pipeline remains the same. In fact, we went beyond the conventional deep learning, and implemented a spiking deep learning algorithm which is not a skill that many people possess. \n",
    "\n",
    "Now that you know the basic implementation, there are lots of directions which you can pursue if you are interested in this research area- how do the hyper-parameters such as the pseudo-gradient window affect the network training process? How much training data do you need to achieve good performance? How does the amount of training data vary with the complexity of the task? To what limits can you push the spike encoding, i.e. what is the minimum timesteps you need to achieve good performance? \n",
    "\n",
    "You do not need to answer these questions in this assignment but it is something to think about if you are interested. \n",
    "\n",
    "**As the concluding question of the course, can you describe what are the three key lessons you learned from the course and why is it important to learn them?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 6\n",
    "Three key lessons I learned from this course is\n",
    "\n",
    "1. This course taught me the application of a Spiking neural network. Instead of the limited knowledge we know about machine learning and how it helps us deal with data/ predict it. SNNs give a more biological perspective of how a spike represents the  information being transmitted only when a membrane potential reaches a specific value as opposed to information being transmitted at each propagation cycle\n",
    "\n",
    "2. I learnt how to implement hyperparameter tuning on a neural network. I realized how tweaking values can improve or reduce the accuracy of a model to a large extent.\n",
    "\n",
    "3. I also learned about biological plausability - Learn about applications & implications of rules like Hebb learning rule, ojas rule, STDP etc.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
